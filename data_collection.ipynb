{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **I. Thu thập dữ liệu (Crawling Data Method)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Thu thập dữ liệu trên **Batdongsan.com**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can install selenium by run the segment code below\n",
    "# !pip install selenium\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary librarys\n",
    "import time\n",
    "import csv\n",
    "import logging\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tempfile\n",
    "import os\n",
    "import random\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "from multiprocessing import Pool, current_process, log_to_stderr, get_logger\n",
    "from multiprocessing.pool import ThreadPool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config logging \n",
    "log_to_stderr()\n",
    "logger = get_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(details):\n",
    "    csv_file = 'property_listings.csv'\n",
    "\n",
    "    file_empty = False\n",
    "    try:\n",
    "        with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            file_empty = len(list(reader)) == 0\n",
    "    except FileNotFoundError:\n",
    "        file_empty = True\n",
    "\n",
    "    with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        if file_empty:\n",
    "            writer.writerow([\n",
    "                'Title', 'Price', 'Address', 'Price per m2', 'Area', \n",
    "                'Bedrooms', 'Toilets', 'Published At', \n",
    "                'Diện tích', 'Mức giá', 'Hướng nhà', 'Hướng ban công',\n",
    "                'Ngày đăng', 'Ngày hết hạn', 'Loại tin', 'Mã tin',\n",
    "                'Project Title', 'Status', 'Number of Apartments',\n",
    "                'Number of Buildings', 'Developer', 'Project Address'\n",
    "            ])\n",
    "\n",
    "        row = [\n",
    "            details.get('Project Title', ''),\n",
    "            details.get('Mức giá', ''),\n",
    "            details.get('Project Address', ''),\n",
    "            details.get('Price per m2', ''),\n",
    "            details.get('Diện tích', ''),\n",
    "            details.get('Số phòng ngủ', ''),\n",
    "            details.get('Số toilet', ''),\n",
    "            details.get('Ngày đăng', ''),\n",
    "            details.get('Diện tích', ''),\n",
    "            details.get('Mức giá', ''),\n",
    "            details.get('Hướng nhà', ''),\n",
    "            details.get('Hướng ban công', ''),\n",
    "            details.get('Ngày đăng', ''),\n",
    "            details.get('Ngày hết hạn', ''),\n",
    "            details.get('Loại tin', ''),\n",
    "            details.get('Mã tin', ''),\n",
    "            details.get('Project Title', ''),\n",
    "            details.get('Status', ''),\n",
    "            details.get('Number of Apartments', ''),\n",
    "            details.get('Number of Buildings', ''),\n",
    "            details.get('Developer', ''),\n",
    "            details.get('Project Address', '')\n",
    "        ]\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_driver():\n",
    "    firefox_options = Options()\n",
    "    firefox_options.add_argument(\"--headless\")\n",
    "    firefox_options.add_argument(\"--no-sandbox\")\n",
    "    firefox_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    firefox_options.binary_location = \"C:\\\\Program Files\\\\Mozilla Firefox\\\\firefox.exe\"\n",
    "    service = Service('./geckodriver-v0.35.0-win32/geckodriver.exe')\n",
    "    return webdriver.Firefox(service=service, options=firefox_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_find_element(driver, locator, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return driver.find_element(*locator)\n",
    "        except StaleElementReferenceException:\n",
    "            time.sleep(1)\n",
    "        except NoSuchElementException:\n",
    "            logging.warning(\"Element not found.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error finding element: {str(e)}\")\n",
    "            break\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_detail_page(url):\n",
    "    driver = create_driver()\n",
    "    property_details = {}\n",
    "    project_info = {}\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 're__pr-specs-content'))\n",
    "        )\n",
    "        logging.info(f\"Accessed URL: {url}, Page Title: {driver.title}\")\n",
    "\n",
    "        specs = driver.find_elements(By.CLASS_NAME, 're__pr-specs-content-item')\n",
    "\n",
    "        for spec in specs:\n",
    "            title_element = safe_find_element(spec, (By.CLASS_NAME, 're__pr-specs-content-item-title'))\n",
    "            value_element = safe_find_element(spec, (By.CLASS_NAME, 're__pr-specs-content-item-value'))\n",
    "            \n",
    "            title = title_element.text.strip() if title_element else 'N/A'\n",
    "            value = value_element.text.strip() if value_element else 'N/A'\n",
    "            \n",
    "            property_details[title] = value\n",
    "\n",
    "        project_card = safe_find_element(driver, (By.CLASS_NAME, 're__project-card-info'))\n",
    "\n",
    "        if project_card:\n",
    "            project_title = safe_find_element(project_card, (By.CLASS_NAME, 're__project-title')).text.strip()\n",
    "            project_info['Project Title'] = project_title\n",
    "\n",
    "            project_status = safe_find_element(project_card, (By.CLASS_NAME, 're__long-text')).text.strip()\n",
    "            project_info['Status'] = project_status\n",
    "\n",
    "            apartments = project_card.find_elements(By.CLASS_NAME, 're__prj-card-config-value')\n",
    "            if len(apartments) > 1:\n",
    "                project_info['Number of Apartments'] = apartments[1].text.strip()\n",
    "            if len(apartments) > 2:\n",
    "                project_info['Number of Buildings'] = apartments[2].text.strip()\n",
    "\n",
    "            developer_name = project_card.find_elements(By.CLASS_NAME, 're__long-text')\n",
    "            if len(developer_name) > 1:\n",
    "                project_info['Developer'] = developer_name[1].text.strip()\n",
    "\n",
    "        short_info = driver.find_elements(By.CLASS_NAME, 're__pr-short-info-item')\n",
    "\n",
    "        for info in short_info:\n",
    "            title = safe_find_element(info, (By.CLASS_NAME, 'title')).text.strip()\n",
    "            value = safe_find_element(info, (By.CLASS_NAME, 'value')).text.strip()\n",
    "            property_details[title] = value\n",
    "\n",
    "        address_element = safe_find_element(driver, (By.CLASS_NAME, 'js__pr-address'))\n",
    "        if address_element:\n",
    "            property_details['Project Address'] = address_element.text.strip()\n",
    "\n",
    "        property_details.update(project_info)\n",
    "        for title, value in property_details.items():\n",
    "            logging.info(f\"{title}: {value}\")\n",
    "\n",
    "        write_to_csv(property_details)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error accessing page {url}: {str(e)}\")\n",
    "        \n",
    "    driver.quit()\n",
    "    return property_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    driver = create_driver()\n",
    "    data = []\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 20).until(EC.presence_of_all_elements_located((By.CLASS_NAME, 're__srp-list')))\n",
    "        logging.info(f\"Accessed URL: {url}, Page Title: {driver.title}\")\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                listings = driver.find_elements(By.CLASS_NAME, 'js__card')\n",
    "                logging.info(f\"Number of listings found: {len(listings)}\")\n",
    "\n",
    "                if not listings:\n",
    "                    logging.info(f'Found 0 listings on {url}')\n",
    "                    break\n",
    "\n",
    "                for listing in listings:\n",
    "                    try:\n",
    "                        detail_link = listing.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                        logging.info(f\"Accessing detail URL: {detail_link}\")\n",
    "\n",
    "                        detail_data = scrape_detail_page(detail_link)\n",
    "                        if detail_data:\n",
    "                            data.append(detail_data)\n",
    "\n",
    "                    except StaleElementReferenceException:\n",
    "                        logging.warning(\"Stale element reference encountered while accessing detail link.\")\n",
    "                        break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        logging.error(f\"Error parsing listing: {e}\")\n",
    "\n",
    "                break\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                logging.warning(\"Stale element reference encountered while fetching listings. Retrying...\")\n",
    "                time.sleep(1)\n",
    "\n",
    "    except TimeoutException:\n",
    "        logging.error(f\"Timeout while accessing page: {url}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error accessing {url}: {str(e)}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(page_number):\n",
    "    base_url = 'https://batdongsan.com.vn/nha-dat-ban-tp-hcm/p'\n",
    "    url = f'{base_url}{page_number}'\n",
    "    logging.info(f'Process {current_process().name} scraping page {page_number}...')\n",
    "    page_data = scrape_page(url)\n",
    "    time.sleep(random.randint(3, 7))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Số lượng trang cần xử lý\n",
    "num_pages = 5\n",
    "\n",
    "# Số luồng\n",
    "num_threads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with ThreadPool(num_threads) as pool:  # Start with 2 processes, increase if stable\n",
    "    pool.map(worker, range(1, num_pages + 1))\n",
    "\n",
    "logging.info('Data saved to property_listings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Thu thập dữ liệu về giá khuyến nghị của chính phủ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_excel(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".xlsx\")\n",
    "        with open(temp_file.name, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        print(\"File downloaded successfully.\")\n",
    "        return temp_file.name\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_to_csv(excel_path, output_csv):\n",
    "    excel_data = None\n",
    "    try:\n",
    "        excel_data = pd.ExcelFile(excel_path)\n",
    "        combined_data = pd.DataFrame()\n",
    "        \n",
    "        for sheet_name in excel_data.sheet_names:\n",
    "            # Load sheet and skip rows until headers\n",
    "            df = excel_data.parse(sheet_name, skiprows=6)\n",
    "\n",
    "            print(\"Crawling...\", sheet_name)\n",
    "            # Check if DataFrame has the expected number of columns\n",
    "            if df.shape[1] < 6:  # Fewer columns than expected\n",
    "                print(f\"Skipping sheet '{sheet_name}' - Fewer columns than expected.\")\n",
    "                continue\n",
    "            \n",
    "            # Rename columns if they match the expected structure\n",
    "            df.columns = df.columns[:6].tolist() + ['extra_column' for _ in range(df.shape[1] - 6)]\n",
    "            df = df.rename(columns={df.columns[1]: 'TÊN ĐƯỜNG', df.columns[5]: 'Giá đất đề nghị điều chỉnh'})\n",
    "            \n",
    "            # Drop rows without a street name\n",
    "            df = df.dropna(subset=['TÊN ĐƯỜNG'])\n",
    "            \n",
    "            # Convert 'Giá đất đề nghị điều chỉnh' to numeric, coerce errors\n",
    "            df['Giá đất đề nghị điều chỉnh'] = pd.to_numeric(df['Giá đất đề nghị điều chỉnh'], errors='coerce')\n",
    "            \n",
    "            # Drop rows with NaN values in 'Giá đất đề nghị điều chỉnh' after conversion\n",
    "            df = df.dropna(subset=['Giá đất đề nghị điều chỉnh'])\n",
    "            \n",
    "            # Add district column\n",
    "            df['QUẬN'] = sheet_name\n",
    "            \n",
    "            # Clone \"THỦ ĐỨC\" to \"Quận 2\" and \"Quận 9\"\n",
    "            if 'THỦ ĐỨC' in df['QUẬN'].values:\n",
    "                df_2 = df[df['QUẬN'] == 'Thủ Đức'].copy()\n",
    "                df_2['QUẬN'] = 'Quận 2'  # Clone for Quận 2\n",
    "                df_9 = df[df['QUẬN'] == 'Thủ Đức'].copy()\n",
    "                df_9['QUẬN'] = 'Quận 9'  # Clone for Quận 9\n",
    "                df = pd.concat([df, df_2, df_9], ignore_index=True)\n",
    "            \n",
    "            # Select relevant columns and ignore extra columns\n",
    "            df_filtered = df[['TÊN ĐƯỜNG', 'Giá đất đề nghị điều chỉnh', 'QUẬN']]\n",
    "            combined_data = pd.concat([combined_data, df_filtered], ignore_index=True)\n",
    "        \n",
    "        if not combined_data.empty:\n",
    "            # Group by 'TÊN ĐƯỜNG' and 'District' and calculate the mean of 'Giá đất đề nghị điều chỉnh'\n",
    "            aggregated_data = combined_data.groupby(['TÊN ĐƯỜNG', 'QUẬN'], as_index=False)['Giá đất đề nghị điều chỉnh'].mean()\n",
    "            \n",
    "            # Sort the aggregated data by 'District' and then by 'Giá đất đề nghị điều chỉnh' in descending order\n",
    "            aggregated_data = aggregated_data.sort_values(by=['QUẬN', 'Giá đất đề nghị điều chỉnh'], ascending=[True, False])\n",
    "            \n",
    "            # Save the sorted aggregated data to the CSV file\n",
    "            aggregated_data.to_csv(output_csv, index=False)\n",
    "            print(f\"Data saved to {output_csv}\")\n",
    "        else:\n",
    "            print(\"No data found with the specified columns.\")\n",
    "    except ImportError as e:\n",
    "        print(\"Missing required module. Please install the required module and try again.\")\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        if excel_data is not None:\n",
    "            excel_data.close()  # Ensure the file is closed after reading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    url = \"https://xdcs.cdnchinhphu.vn/446259493575335936/2024/10/22/phu-luc-bang-8-bgd-dat-o-1-1729562018876764836390-1-17295692360131820457301.xlsx\"\n",
    "    output_csv = \"gbd.csv\"\n",
    "    \n",
    "    excel_path = download_excel(url)\n",
    "    if excel_path:\n",
    "        try:\n",
    "            extract_to_csv(excel_path, output_csv)\n",
    "        finally:\n",
    "            os.remove(excel_path)  # Clean up the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
